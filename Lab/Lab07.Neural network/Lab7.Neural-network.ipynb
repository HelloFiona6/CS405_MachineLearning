{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB7 tutorial for Machine Learning <br >Neural NetWork & Pytorch\n",
    "> The document description are designed by JIa Yanhong in 2022. Oct. 20th\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "- Install the pytorch neural network framework for your computer.\n",
    "- Learn to use pytorch.\n",
    "- Implement a simple neural network using pytorch\n",
    "- Complete the LAB assignment.\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "Neural networks, also known as artificial neural networks (ANNs) or simulated neural networks (SNNs), are a subset of  machine learning and are at the heart of  deep learning algorithms. Their name and structure are inspired by the human brain, mimicking the way that biological neurons signal to one another.\n",
    "\n",
    "![img](images/1hkYlTODpjJgo32DoCOWN5w.png)\n",
    "\n",
    "The above neural could be represented as:\n",
    "$$y=f(b+\\sum_{i=1}^{n}w_ix_i)=f(w^Tx)$$\n",
    "\n",
    "Artificial neural networks (ANNs) are comprised of a node layers, containing `an input layer`, `one or more hidden layers`, and `an output layer`. Each node, or artificial neuron, connects to another and has an associated `weight` and `threshold`. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network.\n",
    "\n",
    "<img src=\"images/ICLH_Diagram_Batch_01_03-DeepNeuralNetwork-WHITEBG.png\" alt=\"Visual diagram of an input layer, hidden layers, and an output layer of a feedforward neural network \" style=\"zoom:40%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The role of neural networks in Machine learning\n",
    "+ Supervise machine learning process "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Supervise machine learning process ](images/Supervise-machine-learning-process.png)\n",
    "\n",
    "  In the figure above, the hardest part is how to obtain valid feature vectors. This is a technique called **feature engineering**.\n",
    "\n",
    "+ **The Importance of Feature Engineering:**\n",
    "\n",
    "  + Preprocessing and feature extraction determine the upper bound of the model\n",
    "  + The algorithm and parameter selection approach this upper bound.\n",
    "\n",
    "+ **Traditional feature extraction methods:**\n",
    "\n",
    "  <img src=\"images/image-20221020212446504.png\" alt=\"image-20221020212446504 \" style=\"zoom:60%;\" />\n",
    "\n",
    "+ **Neural networks automatically extract features**\n",
    "  \n",
    "  <img src=\"images/image-20221020212805853.png\" alt=\"image-20221020212805853 \" style=\"zoom:60%;\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch\n",
    "### Install Pytorch\n",
    "\n",
    "First check your CUDA version installed on your system using Nvidia control panel:\n",
    "\n",
    "<img src=\"images/check_cuda.png\" alt=\"check_cuda\" style=\"zoom:60%;\" />\n",
    "\n",
    "Upgrade your Nvidia driver if your CUDA version is low.\n",
    "\n",
    "Then follow the steps to install Pytorch:\n",
    "\n",
    "https://pytorch.org/get-started/locally/\n",
    "\n",
    "If your computer has no CUDA, don't panic, there are other ways too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning pytorch with linear regression\n",
    "#### Linear regression\n",
    "Do you remember the general form of linear regression model's prediction?\n",
    "$$\\hat{y}=h_{\\theta }(x)=\\theta _{0}+\\theta _{1}x_{1}+\\theta _{2}x_{2}+...+\\theta _{n}x_{n}=\\theta ^{T}\\cdot x$$\n",
    "\n",
    "<center>\n",
    "    <img src='images/fit-linreg.svg' style=\"zoom:100%;\"/>\n",
    "    <br>\n",
    "    <div style=\"\">\n",
    "       Fitting a linear regression model to one-dimensional data\n",
    "    </div>\n",
    "</center>\n",
    "\n",
    "Linear regression is a single-layer neural network, We used this simple network to learn how to use pytorch.\n",
    "<center>\n",
    "    <img src='images/singleneuron.svg' style=\"zoom:100%;\"/>\n",
    "    <br>\n",
    "    <div style=\"\">\n",
    "        Linear regression is a single-layer neural network \n",
    "    </div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Warm-up: [numpy](https://numpy.org/learn/)\n",
    "Before introducing PyTorch, we will first implement the network using numpy.\n",
    "\n",
    "Numpy provides an n-dimensional array object, and many functions for manipulating these arrays. Numpy is a generic framework for scientific computing; it does not know anything about computation graphs, or deep learning, or gradients. However we can easily use numpy to fit a third order polynomial to sine function by manually implementing the forward and backward passes through the network using numpy operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1ba0a27da30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "# Create random input and output data\n",
    "x = np.linspace(-math.pi, math.pi, 2000)\n",
    "y = np.sin(x)\n",
    "\n",
    "plt.scatter(x,y,c = 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 3547.944132515743\n",
      "199 2436.489577945764\n",
      "299 1675.9822822891524\n",
      "399 1155.0103608673098\n",
      "499 797.7187088961246\n",
      "599 552.4023588019154\n",
      "699 383.77752359704783\n",
      "799 267.73871244659705\n",
      "899 187.79838386972298\n",
      "999 132.66650823414602\n",
      "1099 94.60323635490091\n",
      "1199 68.29647129657462\n",
      "1299 50.09621793030301\n",
      "1399 37.49170577142937\n",
      "1499 28.7538959026155\n",
      "1599 22.69078042505072\n",
      "1699 18.47969485113492\n",
      "1799 15.55227411330057\n",
      "1899 13.515435486090276\n",
      "1999 12.097045040079983\n",
      "Result: y = -0.054390701910491744 + 0.8321595524183303 x + 0.009383301144746265 x^2 + -0.08983391450062281 x^3\n"
     ]
    }
   ],
   "source": [
    "# Randomly initialize weights\n",
    "a = np.random.randn()\n",
    "b = np.random.randn()\n",
    "c = np.random.randn()\n",
    "d = np.random.randn()\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    # y = a + b x + c x^2 + d x^3\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # Update weights\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch: Tensors \n",
    "Numpy is a great framework, but it cannot utilize GPUs to accelerate its numerical computations. For modern deep neural networks, GPUs often provide speedups of 50x or greater, so unfortunately numpy won’t be enough for modern deep learning.\n",
    "\n",
    "Here we introduce the most fundamental PyTorch concept: the Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 19\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m(\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m    \"cuda\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     18\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(device)\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mget_device_name(device))\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Create random input and output data\u001b[39;00m\n\u001b[0;32m     22\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m-\u001b[39mmath\u001b[38;5;241m.\u001b[39mpi, math\u001b[38;5;241m.\u001b[39mpi, \u001b[38;5;241m2000\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[1;32md:\\Program\\anaconda\\Lib\\site-packages\\torch\\cuda\\__init__.py:414\u001b[0m, in \u001b[0;36mget_device_name\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_name\u001b[39m(device: Optional[_device_t] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    403\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the name of a device.\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \n\u001b[0;32m    405\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;124;03m        str: the name of the device\u001b[39;00m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_device_properties(device)\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[1;32md:\\Program\\anaconda\\Lib\\site-packages\\torch\\cuda\\__init__.py:444\u001b[0m, in \u001b[0;36mget_device_properties\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_properties\u001b[39m(device: _device_t) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _CudaDeviceProperties:\n\u001b[0;32m    435\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the properties of a device.\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \n\u001b[0;32m    437\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;124;03m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 444\u001b[0m     _lazy_init()  \u001b[38;5;66;03m# will define _get_device_properties\u001b[39;00m\n\u001b[0;32m    445\u001b[0m     device \u001b[38;5;241m=\u001b[39m _get_device_index(device, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count():\n",
      "File \u001b[1;32md:\\Program\\anaconda\\Lib\\site-packages\\torch\\cuda\\__init__.py:284\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    287\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    288\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = 'cpu'\n",
    "'''\n",
    "(\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "'''\n",
    "device = torch.device(device)\n",
    "print(torch.cuda.get_device_name(device))\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Randomly initialize weights\n",
    "a = torch.randn((), device=device, dtype=dtype)\n",
    "b = torch.randn((), device=device, dtype=dtype)\n",
    "c = torch.randn((), device=device, dtype=dtype)\n",
    "d = torch.randn((), device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch: Tensors and autograd\n",
    "In the above examples, we had to manually implement both the forward and backward passes of our neural network. Manually implementing the backward pass is not a big deal for a small two-layer network, but can quickly get very hairy for large complex networks.\n",
    "\n",
    "Thankfully, we can use automatic differentiation to automate the computation of backward passes in neural networks. The autograd package in PyTorch provides exactly this functionality. \n",
    "\n",
    "When using autograd, the forward pass of your network will define a computational graph; nodes in the graph will be Tensors, and edges will be functions that produce output Tensors from input Tensors. Backpropagating through this graph then allows you to easily compute gradients.\n",
    "\n",
    "This sounds complicated, it’s pretty simple to use in practice. Each Tensor represents a node in a computational graph. If `x` is a Tensor that has `x.requires_grad=True` then `x.grad` is another Tensor holding the gradient of `x` with respect to some scalar value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 1661.2891845703125\n",
      "199 1146.617919921875\n",
      "299 793.10888671875\n",
      "399 550.018798828125\n",
      "499 382.67205810546875\n",
      "599 267.3403625488281\n",
      "699 187.77066040039062\n",
      "799 132.81483459472656\n",
      "899 94.81932067871094\n",
      "999 68.52279663085938\n",
      "1099 50.30497741699219\n",
      "1199 37.671600341796875\n",
      "1299 28.902402877807617\n",
      "1399 22.80990982055664\n",
      "1499 18.57327651977539\n",
      "1599 15.624648094177246\n",
      "1699 13.570720672607422\n",
      "1799 12.138849258422852\n",
      "1899 11.139869689941406\n",
      "1999 10.442389488220215\n",
      "Result: y = 0.03906906023621559 + 0.8409804105758667 x + -0.006740061566233635 x^2 + -0.09108860045671463 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "# device = torch.device(\"cuda:0\")\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "\n",
    "# Randomly initialize weights\n",
    "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.data.item())\n",
    "\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    # grad_y_pred = 2.0 * (y_pred - y)\n",
    "    # grad_a = grad_y_pred.sum()\n",
    "    # grad_b = (grad_y_pred * x).sum()\n",
    "    # grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    # grad_d = (grad_y_pred * x ** 3).sum()\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    a.data -= learning_rate * a.grad.data\n",
    "    b.data -= learning_rate * b.grad.data\n",
    "    c.data -= learning_rate * c.grad.data\n",
    "    d.data -= learning_rate * d.grad.data\n",
    "\n",
    "    a.grad.data.zero_()\n",
    "    b.grad.data.zero_()\n",
    "    c.grad.data.zero_()\n",
    "    d.grad.data.zero_()\n",
    "\n",
    "\n",
    "print(f'Result: y = {a.data.item()} + {b.data.item()} x + {c.data.item()} x^2 + {d.data.item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytorch: [nn module](https://pytorch.org/docs/stable/nn.html)\n",
    "we use the nn package to implement our polynomial model network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Using torch.nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 263.47265625\n",
      "199 177.30349731445312\n",
      "299 120.29645538330078\n",
      "399 82.58050537109375\n",
      "499 57.62715148925781\n",
      "599 41.11671447753906\n",
      "699 30.19214630126953\n",
      "799 22.963350296020508\n",
      "899 18.179834365844727\n",
      "999 15.014198303222656\n",
      "1099 12.919175148010254\n",
      "1199 11.532608032226562\n",
      "1299 10.614906311035156\n",
      "1399 10.007426261901855\n",
      "1499 9.605265617370605\n",
      "1599 9.339049339294434\n",
      "1699 9.162799835205078\n",
      "1799 9.046095848083496\n",
      "1899 8.968818664550781\n",
      "1999 8.917638778686523\n",
      "Result: y = -0.001797378878109157 + 0.8471380472183228 x + 0.00031007741927169263 x^2 + -0.0919644683599472 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# For this example, the output y is a linear function of (x, x^2, x^3), so\n",
    "# we can consider it as a linear layer neural network. Let's prepare the\n",
    "# tensor (x, x^2, x^3).\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# In the above code, x.unsqueeze(-1) has shape (2000, 1), and p has shape\n",
    "# (3,), for this case, broadcasting semantics will apply to obtain a tensor\n",
    "# of shape (2000, 3) \n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. The Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "# The Flatten layer flatens the output of the linear layer to a 1D tensor,\n",
    "# to match the shape of `y`.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "\n",
    "# You can access the first layer of `model` like accessing the first item of a list\n",
    "linear_layer = model[0]\n",
    "\n",
    "# For linear layer, its parameters are stored as `weight` and `bias`.\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Define the class\n",
    "  \n",
    "We define our neural network by subclassing `nn.Module`, and initialize the neural network layers in __init__. Every `nn.Module` subclass implements the operations on input data in the `forward()` method.\n",
    "The `forward()` method is in charge of conducting the **forward propagation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearModel, self).__init__()       \n",
    "        self.linear = nn.Linear(3, 1)\n",
    "        self.flatten = nn.Flatten(0, 1)\n",
    "#       self.model = torch.nn.Sequential(\n",
    "#     torch.nn.Linear(3, 1),\n",
    "#     torch.nn.Flatten(0, 1)\n",
    "# )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.flatten(self.linear(x))\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 689.000244140625\n",
      "199 465.609619140625\n",
      "299 315.8400573730469\n",
      "399 215.35231018066406\n",
      "499 147.876708984375\n",
      "599 102.53111267089844\n",
      "699 72.03153991699219\n",
      "799 51.49944305419922\n",
      "899 37.664825439453125\n",
      "999 28.334264755249023\n",
      "1099 22.035308837890625\n",
      "1199 17.778806686401367\n",
      "1299 14.899531364440918\n",
      "1399 12.949874877929688\n",
      "1499 11.628296852111816\n",
      "1599 10.73147964477539\n",
      "1699 10.122252464294434\n",
      "1799 9.707926750183105\n",
      "1899 9.425835609436035\n",
      "1999 9.233548164367676\n",
      "Result: y = 0.015414176508784294 + 0.8428459167480469 x + -0.0026592020876705647 x^2 + -0.09135395288467407 x^3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# For this example, the output y is a linear function of (x, x^2, x^3), so\n",
    "# we can consider it as a linear layer neural network. Let's prepare the\n",
    "# tensor (x, x^2, x^3).\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# In the above code, x.unsqueeze(-1) has shape (2000, 1), and p has shape\n",
    "# (3,), for this case, broadcasting semantics will apply to obtain a tensor\n",
    "# of shape (2000, 3) \n",
    "\n",
    "\n",
    "model = LinearModel()\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "for t in range(2000):\n",
    "\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "\n",
    "# You can access the first layer of `model` like accessing the first item of a list\n",
    "linear_layer = model.linear\n",
    "\n",
    "\n",
    "\n",
    "# For linear layer, its parameters are stored as `weight` and `bias`.\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch: optim\n",
    "Up to this point we have updated the weights of our models by manually mutating the Tensors holding learnable parameters with `torch.no_grad()`. This is not a huge burden for simple optimization algorithms like stochastic gradient descent, but in practice we often train neural networks using more sophisticated optimizers like AdaGrad, RMSProp, Adam, etc.\n",
    "\n",
    "The `optim` package in PyTorch abstracts the idea of an optimization algorithm and provides implementations of commonly used optimization algorithms.\n",
    "\n",
    "In this example we will use the `nn` package to define our model as before, but we will optimize the model using the `RMSprop` algorithm provided by the `optim` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 19920.525390625\n",
      "199 8558.5791015625\n",
      "299 3671.718505859375\n",
      "399 1825.821533203125\n",
      "499 1195.9759521484375\n",
      "599 951.4232177734375\n",
      "699 812.0741577148438\n",
      "799 678.960205078125\n",
      "899 545.4650268554688\n",
      "999 422.737060546875\n",
      "1099 316.32952880859375\n",
      "1199 227.34109497070312\n",
      "1299 155.393798828125\n",
      "1399 99.76029968261719\n",
      "1499 59.398319244384766\n",
      "1599 32.773963928222656\n",
      "1699 17.634563446044922\n",
      "1799 10.967185974121094\n",
      "1899 9.105303764343262\n",
      "1999 8.913334846496582\n",
      "Result: y = 3.674485604676647e-09 + 0.8535340428352356 x + 3.067144760393603e-09 x^2 + -0.09344932436943054 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# For this example, the output y is a linear function of (x, x^2, x^3), so\n",
    "# we can consider it as a linear layer neural network. Let's prepare the\n",
    "# tensor (x, x^2, x^3).\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# In the above code, x.unsqueeze(-1) has shape (2000, 1), and p has shape\n",
    "# (3,), for this case, broadcasting semantics will apply to obtain a tensor\n",
    "# of shape (2000, 3) \n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. The Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "# The Flatten layer flatens the output of the linear layer to a 1D tensor,\n",
    "# to match the shape of `y`.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "model.requires_grad_()\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "optimizer = torch.optim.RMSprop(params=model.parameters(),lr=0.001)\n",
    "#optimizer = torch.optim.SGD(model.parameters(),lr=1e-6,momentum=0.9)\n",
    "\n",
    "for t in range(2000):\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass: compute predicted y by passing x to the model. \n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "   \n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())    \n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. \n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "# You can access the first layer of `model` like accessing the first item of a list\n",
    "linear_layer = model[0]\n",
    "\n",
    "# For linear layer, its parameters are stored as `weight` and `bias`.\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning pytorch with logistic regression\n",
    "If we use a single-layer network for classification, this is known as a logistic regression.\n",
    "\n",
    "\n",
    " We need to add the sigmoid function to the output of the linear regression.\n",
    "<center>\n",
    "    <img src='images/Center.png' style=\"zoom:100%;\"/>\n",
    "    <br>\n",
    "    <div style=\"\">\n",
    "       Perceptron\n",
    "    </div>\n",
    "</center>\n",
    "\n",
    "\n",
    "\n",
    "Let us define the number of epochs and the learning rate we want our model for training. As the data is a binary  classification, we will use **Binary Cross Entropy** as the **loss function** used to optimize the model using an `SGD optimizer`.\n",
    "\n",
    "<font size=5 color='red'>Please complete this part of the code!!</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "### Perceptron-The basic unit of neural network\n",
    "A simple model of a biological neuron in an artificial neural network is known as Perceptron. it is the primary step to learn Machine Learning and Deep Learning technologies.\n",
    "\n",
    "we can consider it as a single-layer neural network with four main parameters, i.e., `input values`, `weights and Bias`, `net sum`, and an `activation function`.\n",
    "\n",
    "![Perceptron in Machine Learning](images/perceptron-in-machine-learning2.png)\n",
    "\n",
    "- **Input Nodes or Input Layer:**\n",
    "\n",
    "This is the primary component of Perceptron which accepts the initial data into the system for further processing.\n",
    "\n",
    "- **Wight and Bias:**\n",
    "\n",
    "Weight parameter represents the strength of the connection between units.  Bias can be considered as the intercept in a linear equation.\n",
    "\n",
    "- **Activation Function:**\n",
    "\n",
    "These are the final and important components that help to determine whether the neuron will fire or not. The activation function of perceptron is `sign function`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron(MLP)\n",
    "A neuron is a mathematical model of the behaviour of a single neuron in a biological nervous system.\n",
    "\n",
    "A single neuron can solve some simple tasks, but the power of neural networks comes when many of them are arranged in layers and connected in a network architecture.\n",
    "\n",
    "<img src=\"images/multilayer-perceptron-1.png\" alt=\"multilayer-perceptron-1 \" style=\"zoom:40%;\" />\n",
    "\n",
    "\n",
    "**A Multilayered Perceptron is a Neural Network**. A neural network having more than 3 hidden layers is called a **Deep Neural Network**.\n",
    "\n",
    "In this lab, Multilayer Perceptron and Neural Network  mean the same thing.\n",
    "\n",
    "------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "\n",
    "Various activation functions that can be used with Perceptron.\n",
    "\n",
    "![Perceptron_36.](images/Perceptron_36.jpg)\n",
    "\n",
    "<font color=\"red\">Neural network without activation functions are simply linear regression model</font>. The activation makes the input capable of learning and performing more complex tasks.\n",
    "\n",
    "![image-20221023014216170](images/image-20221023014216170.png)\n",
    "\n",
    "Therefore, when we write the neural network framework, the neurons in each hidden layer are most of the time **followed by an activation function**.\n",
    "\n",
    "I recommend that you use the relu function as you build your neural network framework.\n",
    "\n",
    "![image-20221023015025204](images/image-20221023015025204.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example for MLP\n",
    "This Example uses dataset digit123.csv , which has 36 columns, and the last column is the dependent variable. We use this dataset to familiarize ourselves with MLP and solve the multi-classification problem.\n",
    "\n",
    "\n",
    "**Note that the values of the dependent variable are 1,2,3, and label coding is required.**\n",
    "#### MLP Model \n",
    "\n",
    "+ step 1 load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9   ...  27  28  29  30  31  32  33  \\\n",
       "0   0   0   0   1   0   0   0   0   0   1  ...   1   0   0   0   0   0   1   \n",
       "1   0   0   0   1   0   0   0   0   1   1  ...   1   0   0   0   0   0   1   \n",
       "2   0   0   1   1   0   0   0   0   0   1  ...   1   0   0   0   0   0   1   \n",
       "3   0   0   0   1   0   0   0   0   0   1  ...   1   0   0   0   0   1   1   \n",
       "4   0   0   0   1   0   0   0   0   0   1  ...   1   0   0   0   0   1   1   \n",
       "\n",
       "   34  35  36  \n",
       "0   0   0   1  \n",
       "1   0   0   1  \n",
       "2   0   0   1  \n",
       "3   0   0   1  \n",
       "4   1   0   1  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 'Matplotlib' is a data visualization library for 2D and 3D plots, built on numpy\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# to suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "# ============================ step 1/6 load datasets ============================\n",
    "df = pd.read_csv(\"datasets/digit123.csv\", header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36\n",
       "1    32\n",
       "2    32\n",
       "3    32\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[36].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 36)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df[36]\n",
    "y.replace((1, 2, 3),(0, 1, 2),inplace=True)\n",
    "X = df.drop(36, axis=1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting for X and Y variables:\n",
    "from sklearn.model_selection import train_test_split\n",
    "## Splitting dataset into 80% Training and 20% Testing Data:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.8, random_state =0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  torch \n",
    "#Converting them to tensors as PyTorch works on, we will use the torch.from_numpy() method:\n",
    "X_train = torch.from_numpy(X_train.values).float()\n",
    "X_test = torch.from_numpy(X_test.values).float()\n",
    "y_train = torch.from_numpy(y_train.values).long()\n",
    "y_test = torch.from_numpy(y_test.values).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ step 2 Define a MLP subclass of nn. Module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a MLP subclass of nn. Module.\n",
    "# ============================ step 2/6 define model ============================\n",
    "import  torch \n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_i, n_h, n_o):\n",
    "        super(MLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(n_i, n_h)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(n_h, n_o)\n",
    "    def forward(self, input):\n",
    "        return self.linear2(self.relu(self.linear1(self.flatten(input))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ step 3 Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 10000\n",
    "learning_rate = 0.001 \n",
    "# Create the model\n",
    "# ============================ step 3/6 Create model ============================\n",
    "models = MLP(X_train.shape[1],X_train.shape[1]//2,y_train.unique().size()[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ step 4 Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================ step 4/6 Loss function ============================\n",
    "criterions = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ step 5 The optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================ step 4/6 The optimizer ============================\n",
    "optimizers = torch.optim.SGD(models.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ step 6 Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20, loss = 1.1009\n",
      "epoch: 40, loss = 1.0969\n",
      "epoch: 60, loss = 1.0929\n",
      "epoch: 80, loss = 1.0891\n",
      "epoch: 100, loss = 1.0853\n",
      "epoch: 120, loss = 1.0815\n",
      "epoch: 140, loss = 1.0778\n",
      "epoch: 160, loss = 1.0741\n",
      "epoch: 180, loss = 1.0705\n",
      "epoch: 200, loss = 1.0669\n",
      "epoch: 220, loss = 1.0635\n",
      "epoch: 240, loss = 1.0601\n",
      "epoch: 260, loss = 1.0567\n",
      "epoch: 280, loss = 1.0534\n",
      "epoch: 300, loss = 1.0501\n",
      "epoch: 320, loss = 1.0469\n",
      "epoch: 340, loss = 1.0437\n",
      "epoch: 360, loss = 1.0405\n",
      "epoch: 380, loss = 1.0373\n",
      "epoch: 400, loss = 1.0342\n",
      "epoch: 420, loss = 1.0310\n",
      "epoch: 440, loss = 1.0279\n",
      "epoch: 460, loss = 1.0248\n",
      "epoch: 480, loss = 1.0217\n",
      "epoch: 500, loss = 1.0186\n",
      "epoch: 520, loss = 1.0156\n",
      "epoch: 540, loss = 1.0125\n",
      "epoch: 560, loss = 1.0095\n",
      "epoch: 580, loss = 1.0064\n",
      "epoch: 600, loss = 1.0034\n",
      "epoch: 620, loss = 1.0003\n",
      "epoch: 640, loss = 0.9972\n",
      "epoch: 660, loss = 0.9942\n",
      "epoch: 680, loss = 0.9911\n",
      "epoch: 700, loss = 0.9880\n",
      "epoch: 720, loss = 0.9849\n",
      "epoch: 740, loss = 0.9818\n",
      "epoch: 760, loss = 0.9787\n",
      "epoch: 780, loss = 0.9756\n",
      "epoch: 800, loss = 0.9724\n",
      "epoch: 820, loss = 0.9693\n",
      "epoch: 840, loss = 0.9661\n",
      "epoch: 860, loss = 0.9629\n",
      "epoch: 880, loss = 0.9597\n",
      "epoch: 900, loss = 0.9565\n",
      "epoch: 920, loss = 0.9533\n",
      "epoch: 940, loss = 0.9500\n",
      "epoch: 960, loss = 0.9468\n",
      "epoch: 980, loss = 0.9435\n",
      "epoch: 1000, loss = 0.9402\n",
      "epoch: 1020, loss = 0.9369\n",
      "epoch: 1040, loss = 0.9336\n",
      "epoch: 1060, loss = 0.9302\n",
      "epoch: 1080, loss = 0.9269\n",
      "epoch: 1100, loss = 0.9235\n",
      "epoch: 1120, loss = 0.9201\n",
      "epoch: 1140, loss = 0.9166\n",
      "epoch: 1160, loss = 0.9132\n",
      "epoch: 1180, loss = 0.9097\n",
      "epoch: 1200, loss = 0.9063\n",
      "epoch: 1220, loss = 0.9028\n",
      "epoch: 1240, loss = 0.8992\n",
      "epoch: 1260, loss = 0.8957\n",
      "epoch: 1280, loss = 0.8921\n",
      "epoch: 1300, loss = 0.8885\n",
      "epoch: 1320, loss = 0.8849\n",
      "epoch: 1340, loss = 0.8813\n",
      "epoch: 1360, loss = 0.8776\n",
      "epoch: 1380, loss = 0.8739\n",
      "epoch: 1400, loss = 0.8702\n",
      "epoch: 1420, loss = 0.8665\n",
      "epoch: 1440, loss = 0.8627\n",
      "epoch: 1460, loss = 0.8590\n",
      "epoch: 1480, loss = 0.8552\n",
      "epoch: 1500, loss = 0.8514\n",
      "epoch: 1520, loss = 0.8475\n",
      "epoch: 1540, loss = 0.8437\n",
      "epoch: 1560, loss = 0.8398\n",
      "epoch: 1580, loss = 0.8359\n",
      "epoch: 1600, loss = 0.8321\n",
      "epoch: 1620, loss = 0.8282\n",
      "epoch: 1640, loss = 0.8242\n",
      "epoch: 1660, loss = 0.8203\n",
      "epoch: 1680, loss = 0.8164\n",
      "epoch: 1700, loss = 0.8124\n",
      "epoch: 1720, loss = 0.8084\n",
      "epoch: 1740, loss = 0.8044\n",
      "epoch: 1760, loss = 0.8004\n",
      "epoch: 1780, loss = 0.7964\n",
      "epoch: 1800, loss = 0.7923\n",
      "epoch: 1820, loss = 0.7882\n",
      "epoch: 1840, loss = 0.7842\n",
      "epoch: 1860, loss = 0.7801\n",
      "epoch: 1880, loss = 0.7760\n",
      "epoch: 1900, loss = 0.7718\n",
      "epoch: 1920, loss = 0.7677\n",
      "epoch: 1940, loss = 0.7636\n",
      "epoch: 1960, loss = 0.7594\n",
      "epoch: 1980, loss = 0.7552\n",
      "epoch: 2000, loss = 0.7510\n",
      "epoch: 2020, loss = 0.7468\n",
      "epoch: 2040, loss = 0.7426\n",
      "epoch: 2060, loss = 0.7384\n",
      "epoch: 2080, loss = 0.7341\n",
      "epoch: 2100, loss = 0.7299\n",
      "epoch: 2120, loss = 0.7256\n",
      "epoch: 2140, loss = 0.7214\n",
      "epoch: 2160, loss = 0.7171\n",
      "epoch: 2180, loss = 0.7128\n",
      "epoch: 2200, loss = 0.7085\n",
      "epoch: 2220, loss = 0.7042\n",
      "epoch: 2240, loss = 0.6999\n",
      "epoch: 2260, loss = 0.6956\n",
      "epoch: 2280, loss = 0.6912\n",
      "epoch: 2300, loss = 0.6869\n",
      "epoch: 2320, loss = 0.6825\n",
      "epoch: 2340, loss = 0.6782\n",
      "epoch: 2360, loss = 0.6738\n",
      "epoch: 2380, loss = 0.6695\n",
      "epoch: 2400, loss = 0.6651\n",
      "epoch: 2420, loss = 0.6607\n",
      "epoch: 2440, loss = 0.6563\n",
      "epoch: 2460, loss = 0.6520\n",
      "epoch: 2480, loss = 0.6476\n",
      "epoch: 2500, loss = 0.6432\n",
      "epoch: 2520, loss = 0.6388\n",
      "epoch: 2540, loss = 0.6344\n",
      "epoch: 2560, loss = 0.6300\n",
      "epoch: 2580, loss = 0.6256\n",
      "epoch: 2600, loss = 0.6212\n",
      "epoch: 2620, loss = 0.6168\n",
      "epoch: 2640, loss = 0.6124\n",
      "epoch: 2660, loss = 0.6080\n",
      "epoch: 2680, loss = 0.6036\n",
      "epoch: 2700, loss = 0.5993\n",
      "epoch: 2720, loss = 0.5949\n",
      "epoch: 2740, loss = 0.5905\n",
      "epoch: 2760, loss = 0.5862\n",
      "epoch: 2780, loss = 0.5818\n",
      "epoch: 2800, loss = 0.5774\n",
      "epoch: 2820, loss = 0.5731\n",
      "epoch: 2840, loss = 0.5687\n",
      "epoch: 2860, loss = 0.5644\n",
      "epoch: 2880, loss = 0.5601\n",
      "epoch: 2900, loss = 0.5557\n",
      "epoch: 2920, loss = 0.5514\n",
      "epoch: 2940, loss = 0.5471\n",
      "epoch: 2960, loss = 0.5428\n",
      "epoch: 2980, loss = 0.5386\n",
      "epoch: 3000, loss = 0.5343\n",
      "epoch: 3020, loss = 0.5301\n",
      "epoch: 3040, loss = 0.5258\n",
      "epoch: 3060, loss = 0.5216\n",
      "epoch: 3080, loss = 0.5174\n",
      "epoch: 3100, loss = 0.5132\n",
      "epoch: 3120, loss = 0.5090\n",
      "epoch: 3140, loss = 0.5049\n",
      "epoch: 3160, loss = 0.5008\n",
      "epoch: 3180, loss = 0.4967\n",
      "epoch: 3200, loss = 0.4926\n",
      "epoch: 3220, loss = 0.4885\n",
      "epoch: 3240, loss = 0.4845\n",
      "epoch: 3260, loss = 0.4805\n",
      "epoch: 3280, loss = 0.4765\n",
      "epoch: 3300, loss = 0.4725\n",
      "epoch: 3320, loss = 0.4686\n",
      "epoch: 3340, loss = 0.4646\n",
      "epoch: 3360, loss = 0.4607\n",
      "epoch: 3380, loss = 0.4568\n",
      "epoch: 3400, loss = 0.4530\n",
      "epoch: 3420, loss = 0.4492\n",
      "epoch: 3440, loss = 0.4453\n",
      "epoch: 3460, loss = 0.4416\n",
      "epoch: 3480, loss = 0.4378\n",
      "epoch: 3500, loss = 0.4341\n",
      "epoch: 3520, loss = 0.4304\n",
      "epoch: 3540, loss = 0.4267\n",
      "epoch: 3560, loss = 0.4230\n",
      "epoch: 3580, loss = 0.4194\n",
      "epoch: 3600, loss = 0.4158\n",
      "epoch: 3620, loss = 0.4122\n",
      "epoch: 3640, loss = 0.4087\n",
      "epoch: 3660, loss = 0.4051\n",
      "epoch: 3680, loss = 0.4016\n",
      "epoch: 3700, loss = 0.3982\n",
      "epoch: 3720, loss = 0.3947\n",
      "epoch: 3740, loss = 0.3913\n",
      "epoch: 3760, loss = 0.3879\n",
      "epoch: 3780, loss = 0.3846\n",
      "epoch: 3800, loss = 0.3812\n",
      "epoch: 3820, loss = 0.3779\n",
      "epoch: 3840, loss = 0.3747\n",
      "epoch: 3860, loss = 0.3714\n",
      "epoch: 3880, loss = 0.3682\n",
      "epoch: 3900, loss = 0.3650\n",
      "epoch: 3920, loss = 0.3619\n",
      "epoch: 3940, loss = 0.3587\n",
      "epoch: 3960, loss = 0.3556\n",
      "epoch: 3980, loss = 0.3526\n",
      "epoch: 4000, loss = 0.3495\n",
      "epoch: 4020, loss = 0.3465\n",
      "epoch: 4040, loss = 0.3435\n",
      "epoch: 4060, loss = 0.3406\n",
      "epoch: 4080, loss = 0.3377\n",
      "epoch: 4100, loss = 0.3348\n",
      "epoch: 4120, loss = 0.3319\n",
      "epoch: 4140, loss = 0.3291\n",
      "epoch: 4160, loss = 0.3262\n",
      "epoch: 4180, loss = 0.3235\n",
      "epoch: 4200, loss = 0.3207\n",
      "epoch: 4220, loss = 0.3180\n",
      "epoch: 4240, loss = 0.3153\n",
      "epoch: 4260, loss = 0.3126\n",
      "epoch: 4280, loss = 0.3100\n",
      "epoch: 4300, loss = 0.3074\n",
      "epoch: 4320, loss = 0.3048\n",
      "epoch: 4340, loss = 0.3022\n",
      "epoch: 4360, loss = 0.2997\n",
      "epoch: 4380, loss = 0.2972\n",
      "epoch: 4400, loss = 0.2947\n",
      "epoch: 4420, loss = 0.2923\n",
      "epoch: 4440, loss = 0.2898\n",
      "epoch: 4460, loss = 0.2875\n",
      "epoch: 4480, loss = 0.2851\n",
      "epoch: 4500, loss = 0.2827\n",
      "epoch: 4520, loss = 0.2804\n",
      "epoch: 4540, loss = 0.2781\n",
      "epoch: 4560, loss = 0.2759\n",
      "epoch: 4580, loss = 0.2736\n",
      "epoch: 4600, loss = 0.2714\n",
      "epoch: 4620, loss = 0.2692\n",
      "epoch: 4640, loss = 0.2671\n",
      "epoch: 4660, loss = 0.2649\n",
      "epoch: 4680, loss = 0.2628\n",
      "epoch: 4700, loss = 0.2607\n",
      "epoch: 4720, loss = 0.2587\n",
      "epoch: 4740, loss = 0.2566\n",
      "epoch: 4760, loss = 0.2546\n",
      "epoch: 4780, loss = 0.2526\n",
      "epoch: 4800, loss = 0.2507\n",
      "epoch: 4820, loss = 0.2487\n",
      "epoch: 4840, loss = 0.2468\n",
      "epoch: 4860, loss = 0.2449\n",
      "epoch: 4880, loss = 0.2430\n",
      "epoch: 4900, loss = 0.2411\n",
      "epoch: 4920, loss = 0.2393\n",
      "epoch: 4940, loss = 0.2375\n",
      "epoch: 4960, loss = 0.2357\n",
      "epoch: 4980, loss = 0.2339\n",
      "epoch: 5000, loss = 0.2322\n",
      "epoch: 5020, loss = 0.2304\n",
      "epoch: 5040, loss = 0.2287\n",
      "epoch: 5060, loss = 0.2270\n",
      "epoch: 5080, loss = 0.2254\n",
      "epoch: 5100, loss = 0.2237\n",
      "epoch: 5120, loss = 0.2221\n",
      "epoch: 5140, loss = 0.2205\n",
      "epoch: 5160, loss = 0.2189\n",
      "epoch: 5180, loss = 0.2173\n",
      "epoch: 5200, loss = 0.2157\n",
      "epoch: 5220, loss = 0.2142\n",
      "epoch: 5240, loss = 0.2127\n",
      "epoch: 5260, loss = 0.2112\n",
      "epoch: 5280, loss = 0.2097\n",
      "epoch: 5300, loss = 0.2082\n",
      "epoch: 5320, loss = 0.2068\n",
      "epoch: 5340, loss = 0.2053\n",
      "epoch: 5360, loss = 0.2039\n",
      "epoch: 5380, loss = 0.2025\n",
      "epoch: 5400, loss = 0.2011\n",
      "epoch: 5420, loss = 0.1997\n",
      "epoch: 5440, loss = 0.1984\n",
      "epoch: 5460, loss = 0.1971\n",
      "epoch: 5480, loss = 0.1957\n",
      "epoch: 5500, loss = 0.1944\n",
      "epoch: 5520, loss = 0.1931\n",
      "epoch: 5540, loss = 0.1919\n",
      "epoch: 5560, loss = 0.1906\n",
      "epoch: 5580, loss = 0.1893\n",
      "epoch: 5600, loss = 0.1881\n",
      "epoch: 5620, loss = 0.1869\n",
      "epoch: 5640, loss = 0.1857\n",
      "epoch: 5660, loss = 0.1845\n",
      "epoch: 5680, loss = 0.1833\n",
      "epoch: 5700, loss = 0.1822\n",
      "epoch: 5720, loss = 0.1810\n",
      "epoch: 5740, loss = 0.1799\n",
      "epoch: 5760, loss = 0.1787\n",
      "epoch: 5780, loss = 0.1776\n",
      "epoch: 5800, loss = 0.1765\n",
      "epoch: 5820, loss = 0.1754\n",
      "epoch: 5840, loss = 0.1744\n",
      "epoch: 5860, loss = 0.1733\n",
      "epoch: 5880, loss = 0.1723\n",
      "epoch: 5900, loss = 0.1712\n",
      "epoch: 5920, loss = 0.1702\n",
      "epoch: 5940, loss = 0.1692\n",
      "epoch: 5960, loss = 0.1682\n",
      "epoch: 5980, loss = 0.1672\n",
      "epoch: 6000, loss = 0.1662\n",
      "epoch: 6020, loss = 0.1652\n",
      "epoch: 6040, loss = 0.1643\n",
      "epoch: 6060, loss = 0.1633\n",
      "epoch: 6080, loss = 0.1624\n",
      "epoch: 6100, loss = 0.1614\n",
      "epoch: 6120, loss = 0.1605\n",
      "epoch: 6140, loss = 0.1596\n",
      "epoch: 6160, loss = 0.1587\n",
      "epoch: 6180, loss = 0.1578\n",
      "epoch: 6200, loss = 0.1570\n",
      "epoch: 6220, loss = 0.1561\n",
      "epoch: 6240, loss = 0.1552\n",
      "epoch: 6260, loss = 0.1544\n",
      "epoch: 6280, loss = 0.1535\n",
      "epoch: 6300, loss = 0.1527\n",
      "epoch: 6320, loss = 0.1519\n",
      "epoch: 6340, loss = 0.1510\n",
      "epoch: 6360, loss = 0.1502\n",
      "epoch: 6380, loss = 0.1494\n",
      "epoch: 6400, loss = 0.1487\n",
      "epoch: 6420, loss = 0.1479\n",
      "epoch: 6440, loss = 0.1471\n",
      "epoch: 6460, loss = 0.1463\n",
      "epoch: 6480, loss = 0.1456\n",
      "epoch: 6500, loss = 0.1448\n",
      "epoch: 6520, loss = 0.1441\n",
      "epoch: 6540, loss = 0.1433\n",
      "epoch: 6560, loss = 0.1426\n",
      "epoch: 6580, loss = 0.1419\n",
      "epoch: 6600, loss = 0.1412\n",
      "epoch: 6620, loss = 0.1405\n",
      "epoch: 6640, loss = 0.1398\n",
      "epoch: 6660, loss = 0.1391\n",
      "epoch: 6680, loss = 0.1384\n",
      "epoch: 6700, loss = 0.1377\n",
      "epoch: 6720, loss = 0.1370\n",
      "epoch: 6740, loss = 0.1364\n",
      "epoch: 6760, loss = 0.1357\n",
      "epoch: 6780, loss = 0.1351\n",
      "epoch: 6800, loss = 0.1344\n",
      "epoch: 6820, loss = 0.1338\n",
      "epoch: 6840, loss = 0.1332\n",
      "epoch: 6860, loss = 0.1325\n",
      "epoch: 6880, loss = 0.1319\n",
      "epoch: 6900, loss = 0.1313\n",
      "epoch: 6920, loss = 0.1307\n",
      "epoch: 6940, loss = 0.1301\n",
      "epoch: 6960, loss = 0.1295\n",
      "epoch: 6980, loss = 0.1289\n",
      "epoch: 7000, loss = 0.1283\n",
      "epoch: 7020, loss = 0.1277\n",
      "epoch: 7040, loss = 0.1272\n",
      "epoch: 7060, loss = 0.1266\n",
      "epoch: 7080, loss = 0.1260\n",
      "epoch: 7100, loss = 0.1255\n",
      "epoch: 7120, loss = 0.1249\n",
      "epoch: 7140, loss = 0.1244\n",
      "epoch: 7160, loss = 0.1238\n",
      "epoch: 7180, loss = 0.1233\n",
      "epoch: 7200, loss = 0.1228\n",
      "epoch: 7220, loss = 0.1222\n",
      "epoch: 7240, loss = 0.1217\n",
      "epoch: 7260, loss = 0.1212\n",
      "epoch: 7280, loss = 0.1207\n",
      "epoch: 7300, loss = 0.1202\n",
      "epoch: 7320, loss = 0.1197\n",
      "epoch: 7340, loss = 0.1192\n",
      "epoch: 7360, loss = 0.1187\n",
      "epoch: 7380, loss = 0.1182\n",
      "epoch: 7400, loss = 0.1177\n",
      "epoch: 7420, loss = 0.1172\n",
      "epoch: 7440, loss = 0.1168\n",
      "epoch: 7460, loss = 0.1163\n",
      "epoch: 7480, loss = 0.1158\n",
      "epoch: 7500, loss = 0.1154\n",
      "epoch: 7520, loss = 0.1149\n",
      "epoch: 7540, loss = 0.1144\n",
      "epoch: 7560, loss = 0.1140\n",
      "epoch: 7580, loss = 0.1135\n",
      "epoch: 7600, loss = 0.1131\n",
      "epoch: 7620, loss = 0.1127\n",
      "epoch: 7640, loss = 0.1122\n",
      "epoch: 7660, loss = 0.1118\n",
      "epoch: 7680, loss = 0.1114\n",
      "epoch: 7700, loss = 0.1109\n",
      "epoch: 7720, loss = 0.1105\n",
      "epoch: 7740, loss = 0.1101\n",
      "epoch: 7760, loss = 0.1097\n",
      "epoch: 7780, loss = 0.1093\n",
      "epoch: 7800, loss = 0.1089\n",
      "epoch: 7820, loss = 0.1085\n",
      "epoch: 7840, loss = 0.1081\n",
      "epoch: 7860, loss = 0.1077\n",
      "epoch: 7880, loss = 0.1073\n",
      "epoch: 7900, loss = 0.1069\n",
      "epoch: 7920, loss = 0.1065\n",
      "epoch: 7940, loss = 0.1061\n",
      "epoch: 7960, loss = 0.1057\n",
      "epoch: 7980, loss = 0.1053\n",
      "epoch: 8000, loss = 0.1050\n",
      "epoch: 8020, loss = 0.1046\n",
      "epoch: 8040, loss = 0.1042\n",
      "epoch: 8060, loss = 0.1039\n",
      "epoch: 8080, loss = 0.1035\n",
      "epoch: 8100, loss = 0.1031\n",
      "epoch: 8120, loss = 0.1028\n",
      "epoch: 8140, loss = 0.1024\n",
      "epoch: 8160, loss = 0.1021\n",
      "epoch: 8180, loss = 0.1017\n",
      "epoch: 8200, loss = 0.1014\n",
      "epoch: 8220, loss = 0.1010\n",
      "epoch: 8240, loss = 0.1007\n",
      "epoch: 8260, loss = 0.1004\n",
      "epoch: 8280, loss = 0.1000\n",
      "epoch: 8300, loss = 0.0997\n",
      "epoch: 8320, loss = 0.0994\n",
      "epoch: 8340, loss = 0.0990\n",
      "epoch: 8360, loss = 0.0987\n",
      "epoch: 8380, loss = 0.0984\n",
      "epoch: 8400, loss = 0.0981\n",
      "epoch: 8420, loss = 0.0977\n",
      "epoch: 8440, loss = 0.0974\n",
      "epoch: 8460, loss = 0.0971\n",
      "epoch: 8480, loss = 0.0968\n",
      "epoch: 8500, loss = 0.0965\n",
      "epoch: 8520, loss = 0.0962\n",
      "epoch: 8540, loss = 0.0959\n",
      "epoch: 8560, loss = 0.0956\n",
      "epoch: 8580, loss = 0.0953\n",
      "epoch: 8600, loss = 0.0950\n",
      "epoch: 8620, loss = 0.0947\n",
      "epoch: 8640, loss = 0.0944\n",
      "epoch: 8660, loss = 0.0941\n",
      "epoch: 8680, loss = 0.0938\n",
      "epoch: 8700, loss = 0.0935\n",
      "epoch: 8720, loss = 0.0932\n",
      "epoch: 8740, loss = 0.0929\n",
      "epoch: 8760, loss = 0.0927\n",
      "epoch: 8780, loss = 0.0924\n",
      "epoch: 8800, loss = 0.0921\n",
      "epoch: 8820, loss = 0.0918\n",
      "epoch: 8840, loss = 0.0916\n",
      "epoch: 8860, loss = 0.0913\n",
      "epoch: 8880, loss = 0.0910\n",
      "epoch: 8900, loss = 0.0908\n",
      "epoch: 8920, loss = 0.0905\n",
      "epoch: 8940, loss = 0.0902\n",
      "epoch: 8960, loss = 0.0900\n",
      "epoch: 8980, loss = 0.0897\n",
      "epoch: 9000, loss = 0.0894\n",
      "epoch: 9020, loss = 0.0892\n",
      "epoch: 9040, loss = 0.0889\n",
      "epoch: 9060, loss = 0.0887\n",
      "epoch: 9080, loss = 0.0884\n",
      "epoch: 9100, loss = 0.0882\n",
      "epoch: 9120, loss = 0.0879\n",
      "epoch: 9140, loss = 0.0877\n",
      "epoch: 9160, loss = 0.0874\n",
      "epoch: 9180, loss = 0.0872\n",
      "epoch: 9200, loss = 0.0869\n",
      "epoch: 9220, loss = 0.0867\n",
      "epoch: 9240, loss = 0.0865\n",
      "epoch: 9260, loss = 0.0862\n",
      "epoch: 9280, loss = 0.0860\n",
      "epoch: 9300, loss = 0.0858\n",
      "epoch: 9320, loss = 0.0855\n",
      "epoch: 9340, loss = 0.0853\n",
      "epoch: 9360, loss = 0.0851\n",
      "epoch: 9380, loss = 0.0848\n",
      "epoch: 9400, loss = 0.0846\n",
      "epoch: 9420, loss = 0.0844\n",
      "epoch: 9440, loss = 0.0842\n",
      "epoch: 9460, loss = 0.0839\n",
      "epoch: 9480, loss = 0.0837\n",
      "epoch: 9500, loss = 0.0835\n",
      "epoch: 9520, loss = 0.0833\n",
      "epoch: 9540, loss = 0.0831\n",
      "epoch: 9560, loss = 0.0829\n",
      "epoch: 9580, loss = 0.0826\n",
      "epoch: 9600, loss = 0.0824\n",
      "epoch: 9620, loss = 0.0822\n",
      "epoch: 9640, loss = 0.0820\n",
      "epoch: 9660, loss = 0.0818\n",
      "epoch: 9680, loss = 0.0816\n",
      "epoch: 9700, loss = 0.0814\n",
      "epoch: 9720, loss = 0.0812\n",
      "epoch: 9740, loss = 0.0810\n",
      "epoch: 9760, loss = 0.0808\n",
      "epoch: 9780, loss = 0.0806\n",
      "epoch: 9800, loss = 0.0804\n",
      "epoch: 9820, loss = 0.0802\n",
      "epoch: 9840, loss = 0.0800\n",
      "epoch: 9860, loss = 0.0798\n",
      "epoch: 9880, loss = 0.0796\n",
      "epoch: 9900, loss = 0.0794\n",
      "epoch: 9920, loss = 0.0792\n",
      "epoch: 9940, loss = 0.0790\n",
      "epoch: 9960, loss = 0.0788\n",
      "epoch: 9980, loss = 0.0786\n",
      "epoch: 10000, loss = 0.0784\n"
     ]
    }
   ],
   "source": [
    "#Train the Model\n",
    "# ============================ step 5/6 training ============================    \n",
    "for epoch in range(num_epochs):\n",
    "    models.train()\n",
    "    optimizers.zero_grad()\n",
    "    # Forward pass\n",
    "    y_pred  = models(X_train)\n",
    "    # Compute Loss\n",
    "    loss = criterions(y_pred, y_train)\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizers.step()\n",
    "    if (epoch+1) % 20 == 0:                                         \n",
    "        # printing loss values on every 10 epochs to keep track\n",
    "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, \n",
    "+ when you call `models(X_train)`, you automatically call `models.forward()` to propagate forward.\n",
    "+  Next, the loss is calculated. When `loss.backward()` is called, it computes the loss gradient with respect to the weights (of the layer). \n",
    "+ The weights are then updated by calling `optimizer.step()`. \n",
    "+ After this, the weights have to be emptied for the next iteration. So the `zero_grad()` method is called.\n",
    "\n",
    "The above code prints the loss at each 20th epoch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ step 7 Model Performance\n",
    "  \n",
    "Let us finally see the model accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9000\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = models(X_test)\n",
    "    y_pred = nn.Softmax(dim=1)(logits)\n",
    "    y_predicted_cls = y_pred.argmax(1)\n",
    "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'accuracy: {acc.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.78      0.88         9\n",
      "           1       0.80      1.00      0.89         4\n",
      "           2       0.88      1.00      0.93         7\n",
      "\n",
      "    accuracy                           0.90        20\n",
      "   macro avg       0.89      0.93      0.90        20\n",
      "weighted avg       0.92      0.90      0.90        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_predicted_cls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB Assignment\n",
    "\n",
    "<font color=red>DDL: week 11.</font>\n",
    "\n",
    "### Exercise 1 logistic regression\n",
    "This exercise uses dataset digit01.csv , which has 13 columns, and the last column is the dependent variable. \n",
    "\n",
    "This part requires you to implement a `logistic regression` using the pytorch framework (defining a logistic regression class that inherits `nn.module`). To test your model, we provide a dataset `digit01.csv` which is in the **datasets folder**. This dataset requires you to divide the training set and the test set by yourself, and it is recommended that 80% of the training set and 20% of the test set be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2  Handwriting recognition with MLP\n",
    "\n",
    "Like last week's lab , your task in this section is also about recognizing handwritten digits, but you are required to use MLP to complete the exercise. It is recommended that you define an MLP class, which is a subclass of `nn.module`.\n",
    "\n",
    "For this exercise we use the `minist` dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
